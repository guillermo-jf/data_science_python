{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPLpSjadj0XA7ErA472Y2ja",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/guillermo-jf/data_science_python/blob/main/DS_python_M3_Load_Data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting started with Accessing Data in Jupyter\n",
        "The most popular data sources that are widely used across the industries to access the data are:\n",
        "\n",
        "Files: CSV, Excel, JSON, TXTs, and Pickle, that can store structured and unstructured data. These files are stored in local or cloud storage and can be accessed using Python in Jupyter.\n",
        "\n",
        "Databases and Data warehouse: Data can be stored in different relational and non-relational databases and data warehouses such as MySQL, PostgreSQL, MongoDB, Redshift, and Snowflake.\n",
        "\n",
        "APIs: Data can also be accessed through APIs from multiple endpoints that sometimes may or may not need authentication.\n",
        "\n",
        "Dataset Repositories: Finally, different platforms provide open source datasets for ML and AI tasks such as Kaggle, Data.world, or Hugging Face. You can load data directly from these platforms.\n",
        "\n",
        "## Installing Necessary Dependencies\n",
        "Python provides a list of dependencies to develop different solutions with ease. For loading the data in Python, one most important libraries is Pandas which helps you load data from multiple data sources. You can install pandas using PIP as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "86ojUMsgjpf9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q0c3Cxzyfn95"
      },
      "outputs": [],
      "source": [
        "$ pip install pandas"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "For loading the data from the APIs or websites you will also require the requests package which can be downloaded as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "gpcxTEqhkIFG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install requests"
      ],
      "metadata": {
        "id": "Is7bBX4tkPd2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Data from Files\n",
        "Using these data file formats such as CSV, JSON, Excel, or text files brings simplicity in saving and loading data and improves the portability of data. The downsides to these formats is that they may not be optimal for handling large-scale datasets, lack built-in support for complex data structures or relationships, and can sometimes suffer from inefficiencies in terms of read/write speeds compared to binary or database storage formats.\n",
        "\n",
        "The pandas library from Python allows you to access the data from all major file types along with different exploration and visualization options.\n",
        "\n",
        "Let’s start with importing pandas that we’ll use throughout this article.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ERsrPpbFkD9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependency\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "Ni9CQIHRk2qm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Data from Excel Files\n",
        "Excel files are used to store tabular data and can have more than one sheet to store different relational tables. Excel (or Google Sheets) is incredibly powerful and common in business, so analysts might find a file given to them for analysis. Pandas provides the read_excel() method to load the data from Excel files. You can load different sheets of data from Excel into the Jupyter environment.\n",
        "\n",
        "Note: Every time you load the data from any file type, it is always loaded as a DataFrame, a data structure of Pandas for storing the tabular or relational data.\n",
        "\n",
        "An example of loading the iris data Excel file may look like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "EZ5Kop4ak3iy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# excel absolute file path\n",
        "excel_file = 'Datasets/iris.xls'\n",
        "\n",
        "# read data from an excel file\n",
        "iris_xl_data = pd.read_excel(excel_file, sheet_name='Data')\n",
        "\n",
        "# check the first few rows of data loaded from Excel\n",
        "iris_xl_data.head()"
      ],
      "metadata": {
        "id": "u-U3NoGalB8Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: You can specify the absolute or relative path of files in any pandas read data function.\n",
        "\n",
        "As you can observe, we have specified the sheet_name that tells the Pandas to load the data from a specific Excel sheet. Other arguments of the read_excel() method you might want to use are:\n",
        "\n",
        "**header:** Row (0-indexed) to use for the column labels. By default, it is inferred, but you can set it to None if there's no header.\n",
        "\n",
        "**names:** This is a list corresponding to the column names. Use this if you want to override or set column names.\n",
        "\n",
        "**usecols:** A list of columns or column range (e.g., 'A:D') to import from the Excel file.\n",
        "\n",
        "**dtype:** Dict of column name to data type. If not provided, pandas will try to infer the data type.\n",
        "\n",
        "## Reading Data from CSV Files\n",
        "CSV files are mainly used for storing relational data and are quick and easy to use with Python and Jupyter. Pandas provides the read_csv() function that can load the data in any environment and provides features to analyze and visualize the loaded data. A sample code to load the iris CSV file using Pandas will look like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "7I6cqDy-lE9t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# csv file path\n",
        "csv_file = 'Datasets/iris.csv'\n",
        "\n",
        "# load data from the CSV file\n",
        "iris_csv_data = pd.read_csv(csv_file)\n",
        "\n",
        "# check the first few rows of data loaded from CSV\n",
        "iris_csv_data.head()"
      ],
      "metadata": {
        "id": "mOyx9FI8lXCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#The read_csv() function also supports different arguments to:\n",
        "\n",
        "#Load specific columns:\n",
        "df = pd.read_csv('filename.csv', usecols=['column1', 'column2'])\n",
        "\n",
        "#Skip erroneous rows:\n",
        "df = pd.read_csv('filename.csv', error_bad_lines=False)\n",
        "\n",
        "#Break files into chunks:\n",
        "chunk_iter = pd.read_csv('filename.csv', chunksize=10000)"
      ],
      "metadata": {
        "id": "Zxgyaphzlr-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reading Data from JSON Files\n",
        "JSON is a simple, lightweight, and widely used data format that stores data in human-readable format. It uses key-value pairs where each key is a string that uniquely identifies a value, which can be a string, number, boolean, array, or another JSON object. This format is commonly used for configuration files, responses from APIs, and data storage in various applications and is well suited for storing small-size dataset features.\n",
        "\n",
        "Pandas provide the read_json() method that helps you load the JSON files into a Dataframe. An example to load the iris.json file using pandas will look like this:\n",
        "\n"
      ],
      "metadata": {
        "id": "Ia5jay75lX8p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# JSON file path\n",
        "json_file = 'Datasets/iris.json'\n",
        "\n",
        "# load data from the JSON file\n",
        "iris_json_data = pd.read_json(json_file)\n",
        "\n",
        "# check the first 5 rows of data loaded from JSON\n",
        "iris_json_data.head()"
      ],
      "metadata": {
        "id": "kFc64mUhml6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Data From Databases and Data Warehouses\n",
        "While file storage is good for small-size datasets, databases, and data warehouses are designed to hold almost any size of data. Most organizations store their large amount of business-specific data in local or cloud databases such as MySQL, MongoDB, or PostgreSQL.\n",
        "\n",
        "These databases are normally created, accessed, and managed using the Structured Query Language (SQL) for different business purposes. Python provides a lot of connectors to access the data from different types of databases using the same SQL queries. Again, pandas play a key role as you can read the queried data as a dataframe and perform multiple operations of your choice.\n",
        "\n"
      ],
      "metadata": {
        "id": "U_Xe0FXPosLn"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zPcRyzB-orYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Data with APIs\n",
        "If data is coming from a source outside your organization, you’ll need to access the data using APIs. The process starts with sending a request containing different query parameters and access credentials to the endpoint (the platform that contains the data).\n",
        "\n",
        "When the credentials and query parameters are validated, the endpoint returns a response such as response code and the data you wanted to access (usually in JSON format) otherwise it will throw an error.\n",
        "\n",
        "When working on Python, the requests module helps you access the data using APIs of different platforms in the Jupyter environment.\n",
        "\n",
        "### Accessing Data with APIs without Credentials\n",
        "Sometimes data can be accessed without any credentials using public APIs. A simple example of reading the JSON data from a URL using the get() method from the requests library will look like this:"
      ],
      "metadata": {
        "id": "oCbHegnKo6FF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import requests\n",
        "\n",
        "# Specify the URL where the dataset is hosted\n",
        "url = \"<https://raw.githubusercontent.com/gouravsinghbais/Accessing-Data-in-Jupyter/master/Dataset/iris.json>\"\n",
        "\n",
        "# Send an HTTP GET request to the specified URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Print the HTTP status code (e.g., 200 for \"OK\", 404 for \"Not Found\")\n",
        "print(response.status_code)\n",
        "\n",
        "# Print the JSON content of the response\n",
        "print(response.json())\n",
        "\n",
        "# Convert the JSON response to a pandas DataFrame\n",
        "res = pd.DataFrame(response.json())\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "res.head()"
      ],
      "metadata": {
        "id": "TxdQPsjSo_JF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here, you only need a URL to access the JSON data. The response has two parts to it, one is the response code and the other is the JSON body that contains the data. The JSON data can be read directly as a dataframe using the Pandas module.\n",
        "\n",
        "### Accessing Data with APIs with Credentials\n",
        "Often data will be protected and require specific credentials to authenticate your identity. You may also want to request the specific data for which some additional query parameters can be required. The same get() method can be used here as well where you can also pass the additional parameters as follows:"
      ],
      "metadata": {
        "id": "KK4XK2i4pINE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the necessary libraries\n",
        "import requests\n",
        "\n",
        "# Define the parameters for the API request\n",
        "parameters = {\n",
        "    \"personal_api_key\": \"YOUR_API_KEY\",  # Replace with your actual API key\n",
        "    \"date\": \"2021-09-22\"                 # The date for which you want data\n",
        "}\n",
        "\n",
        "# Send an HTTP GET request to the specified URL with the given parameters\n",
        "response = requests.get(url, params=parameters)\n",
        "\n",
        "# Print the HTTP status code (e.g., 200 for \"OK\", 404 for \"Not Found\")\n",
        "print(response.status_code)\n",
        "\n",
        "# Print the JSON content of the response\n",
        "print(response.json())\n",
        "\n",
        "# Convert the JSON response to a pandas DataFrame\n",
        "res = pd.DataFrame(response.json())\n",
        "\n",
        "# Display the first 5 rows of the DataFrame\n",
        "res.head()"
      ],
      "metadata": {
        "id": "uA7jTcqQpQ6s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing Data with Dataset Access Libraries\n",
        "As data science and machine learning are growing at a rapid pace and more and more models are being deployed to production, there is a need to have a wide range of datasets for experimentation and model testing. Different platforms have sprung up to host open source datasets for a variety of use cases including Classification, Object Detection, and speech recognition. Some of the most popular platforms are Kaggle, Data.world, Hugging Face, and DataCommons.\n",
        "\n",
        "It is possible to access data from these platforms using Python and their respective Python dependencies.\n",
        "\n",
        "## Accessing Data from Kaggle\n",
        "Kaggle is a popular online platform for data science and machine learning that hosts a lot of ML competitions, has a wide range of datasets, and provides a Jupyter Kernel to run notebooks. If you need any dataset for experimentation or even for a business use case, Kaggle probably has something useful. Python provides the kaggle library that lets you interact with the Kaggle platform, and load data from there.\n",
        "\n",
        "You need to make sure that you have the kaggle.json file downloaded from Kaggle. Also, you can download the kaggle Python dependency using PIP as follows:\n",
        "\n"
      ],
      "metadata": {
        "id": "iPlpkEqqpd17"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install kaggle"
      ],
      "metadata": {
        "id": "SUo7wJ_cplgn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# you need to authenticate using the Kaggle API as follows:\n",
        "\n",
        "# Import the Kaggle API client from the kaggle library\n",
        "from kaggle.api.kaggle_api_extended import KaggleApi\n",
        "\n",
        "# Instantiate the KaggleApi object\n",
        "api = KaggleApi()\n",
        "\n",
        "# Authenticate using credentials (typically loaded from ~/.kaggle/kaggle.json)\n",
        "api.authenticate()"
      ],
      "metadata": {
        "id": "O63p31Hrpv9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the authentication is successful, you can use the following code to list out different competitions that you have participated in:\n"
      ],
      "metadata": {
        "id": "jxXZarWMpmbJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# list competitions\n",
        "api.competitions_list(category='gettingStarted')"
      ],
      "metadata": {
        "id": "aBE19jElp1lz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Competition List\n",
        "\n",
        "#If you want to access the data from a specific competition, just provide the name of the competition in the competition_list_files() function.\n",
        "\n",
        "# choose a specific competition\n",
        "api.competition_download_files('titanic')"
      ],
      "metadata": {
        "id": "iZeju96rp9RW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Accessing financial data using pandas_datareader\n",
        "If you need specific financial and economic data, pandas_datareader pulls data Yahoo Finance, Google Finance, Federal Reserve Economic Data (FRED), and World Bank. It is an extension to the most popular pandas library and simplifies the process of accessing time series data like stock prices, economic indicators, and more.\n",
        "\n",
        "You can download the dependency using PIP as follows:"
      ],
      "metadata": {
        "id": "J2Y7iDMDqJRO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "$ pip install pandas_datareader"
      ],
      "metadata": {
        "id": "7QCOAb6Qp_57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once downloaded, you can use the following code to access the Gold stock price from Yahoo as follows:"
      ],
      "metadata": {
        "id": "4l-AgOnwqOCE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import dependency\n",
        "from pandas_datareader import dataimport datetime as dt\n",
        "\n",
        "# access gold price between 2019-01-01 to today\n",
        "zm = data.get_data_yahoo(\n",
        "\t\"GOLD\",\n",
        "\tstart='2019-1-1',\n",
        "\tend=dt.datetime.today()).reset_index()\n",
        "\n",
        "# show the first few rows of data\n",
        "zm.head()"
      ],
      "metadata": {
        "id": "rQNN-gAMqSjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Access any data with Python and Jupyter\n",
        "Python and Jupyter streamlines the process of sourcing and analyzing data. With the plethora of libraries and tools available, accessing diverse datasets is incredibly easy. Whether you're pulling financial data, querying databases, or leveraging platforms like Kaggle, the combined power of Python and Jupyter ensures a better experience for accessing data for analysts."
      ],
      "metadata": {
        "id": "8AoTo5nzqaKW"
      }
    }
  ]
}